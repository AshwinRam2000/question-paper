{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport logging\nimport pandas as pd\nimport numpy as np\nfrom numpy import random\nimport gensim\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport re\nfrom bs4 import BeautifulSoup\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gensim\nwv = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\", binary=True)\nwv.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_averaging(wv, words):\n    all_words, mean = set(), []\n    \n    for word in words:\n        if isinstance(word, np.ndarray):\n            mean.append(word)\n        elif word in wv.vocab:\n            mean.append(wv.syn0norm[wv.vocab[word].index])\n            all_words.add(wv.vocab[word].index)\n\n    if not mean:\n        logging.warning(\"cannot compute similarity with no input %s\", words)\n        # FIXME: remove these examples in pre-processing\n        return np.zeros(wv.vector_size,)\n\n    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n    return mean\n\ndef  word_averaging_list(wv, text_list):\n    return np.vstack([word_averaging(wv, post) for post in text_list ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef w2v_tokenize_text(text):\n    print(text)\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        print(sent)\n        for word in nltk.word_tokenize(sent):\n            print(word)\n\n            if len(word) < 2:\n                continue\n            tokens.append(word)\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/finalset/blooms.csv\",error_bad_lines=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(df.columns[[0]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l=literal_eval(ans[0])\nl=[l]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extractDigits(lst): \n    res = [] \n    print(lst)\n    for el in lst: \n        sub = el.split(', ') \n        res.append(sub) \n      \n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[i for i in Train_X]\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\n\nfrom ast import literal_eval\n# ans = [i for i in d[0]]\n# print (ans)\n# ans[1]\n# # [i for i in ans]\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nporter=PorterStemmer()\ntext=[literal_eval(i) for i in d]\n\n\ntt=[[str(j) for j in i[0]] for i in text]\nt=[]\nfor u in range(0,len(tt)):\n    t.append([tt[u]])\n# tt2=[j for j in tt[0]]\n\n# print(X_train)\n# print(tt)\n\nwordList= []\ni1=0\n# w[1000]=[]\nw={}\nfor i in tt:\n#     wordList.append([])\n    w[i1]=[]\n    for j in i:\n        w[i1].append(porter.stem(j))\n    i1=i1+1\n        \n# wordList\n# ans=[]\nans=[str(en) for en in w.values()]\n# tt2\nl=[]\nfor i in range(0,len(ans)):\n    l.append(str([literal_eval(ans[0])]))\n    \n\n\n# d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_X, Test_X, Train_Y, Test_Y = train_test_split(df['text_final'],df['label'],test_size=0.3)\nEncoder = LabelEncoder()\nTrain_Y = Encoder.fit_transform(Train_Y)\nTest_Y = Encoder.fit_transform(Test_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nimport gensim\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\n\ndef label_sentences(corpus, label_type):\n \n    labeled = []\n    for i, v in enumerate(corpus):\n        label = label_type + '_' + str(i)\n        labeled.append(TaggedDocument(v.split(), [label]))\n    return labeled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df[\"text_final\"], df[\"label\"], random_state=0, test_size=0.3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[]\nd=[i for i in X_train]\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\n\nfrom ast import literal_eval\n# ans = [i for i in d[0]]\n# print (ans)\n# ans[1]\n# # [i for i in ans]\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nporter=PorterStemmer()\nlemmatizer = WordNetLemmatizer()\ntext=[literal_eval(i) for i in d]\n\n\ntt=[[str(j) for j in i[0]] for i in text]\nt=[]\nfor u in range(0,len(tt)):\n    t.append([tt[u]])\n# tt2=[j for j in tt[0]]\n\n# print(X_train)\n# print(tt)\n\nwordList= []\ni1=0\n# w[1000]=[]\nw={}\nfor i in tt:\n#     print(i)\n#     i = ngrams(i,3)\n    \n\n#     wordList.append([])\n    w[i1]=[]\n    for j in i:\n        \n        h=wordnet_lemmatizer.lemmatize(j)\n        \n        w[i1].append(porter.stem(h))\n#          w[i1].append(j)\n    i1=i1+1\n        \n# wordList\n# ans=[]\nans=[str(en) for en in w.values()]\n# tt2\nl=[]\nfor i in range(0,len(ans)):\n    l.append(str([literal_eval(ans[i])]))\n    \n\nX_train=l\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[]\nd=[i for i in X_test]\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\n\nfrom ast import literal_eval\n# ans = [i for i in d[0]]\n# print (ans)\n# ans[1]\n# # [i for i in ans]\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nporter=PorterStemmer()\nlemmatizer = WordNetLemmatizer()\ntext=[literal_eval(i) for i in d]\n\n\ntt=[[str(j) for j in i[0]] for i in text]\nt=[]\nfor u in range(0,len(tt)):\n    t.append([tt[u]])\n# tt2=[j for j in tt[0]]\n\n# print(X_train)\n# print(tt)\n\nwordList= []\ni1=0\n# w[1000]=[]\nw={}\nfor i in tt:\n#     print(i)\n#     i = ngrams(i,3)\n    \n\n#     wordList.append([])\n    w[i1]=[]\n    for j in i:\n        \n        h=wordnet_lemmatizer.lemmatize(j)\n        \n        w[i1].append(porter.stem(h))\n#          w[i1].append(j)\n    i1=i1+1\n        \n# wordList\n# ans=[]\nans=[str(en) for en in w.values()]\n# tt2\nl=[]\nfor i in range(0,len(ans)):\n    l.append(str([literal_eval(ans[i])]))\n    \n\nX_test=l\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = label_sentences(X_train, 'Train')\nX_test = label_sentences(X_test, 'Test')\nall_data = X_train + X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\nmodel_dbow.build_vocab([x for x in tqdm(all_data)])\n\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vectors(model, corpus_size, vectors_size, vectors_type):\n    \"\"\"\n    Get vectors from trained doc2vec model\n    :param doc2vec_model: Trained Doc2Vec model\n    :param corpus_size: Size of the data\n    :param vectors_size: Size of the embedding vectors\n    :param vectors_type: Training or Testing vectors\n    :return: list of vectors\n    \"\"\"\n    vectors = np.zeros((corpus_size, vectors_size))\n    for i in range(0, corpus_size):\n        prefix = vectors_type + '_' + str(i)\n        vectors[i] = model.docvecs[prefix]\n    return vectors\n    \ntrain_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\ntest_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(train_vectors_dbow, y_train)\n\nlogreg = logreg.fit(train_vectors_dbow, y_train)\ny_pred = logreg.predict(test_vectors_dbow)\nfrom sklearn.metrics import classification_report\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 1000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize.fit_on_texts(Train_X) # only fit on train\nx_train = tokenize.texts_to_matrix(Train_X)\nx_test = tokenize.texts_to_matrix(Test_X)\nencoder = LabelEncoder()\nencoder.fit(Train_Y)\ny_train = encoder.transform(Train_Y)\ny_test = encoder.transform(Test_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nepochs = 2\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(1000,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(6))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n              \nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=10,\n                    verbose=1,\n                    validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test, y_test,\n                       batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs=range(1,11)\nplt.plot(epochs,loss_train,'g',label=\"Training accuracy\")\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,11)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Dense(512, input_shape=(1000,)))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(6))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KerasClassifier(build_fn=create_model, verbose=0)\nbatch_size = [10, 20, 40, 60, 80, 100]\nepochs = [10, 50, 100]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pipeline & Gridsearch setup\n# TFIDF pipeline setup\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\ntvc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()),\n                (\"mb\", MultinomialNB()),\n              ])\n# Randomforest pipeline setup\nrf_pipe = Pipeline([(\"tvec\", TfidfVectorizer()),\n                     (\"rf\", RandomForestClassifier())\n                    ])\n# Fit\ntvc_pipe.fit(Train_X, Train_Y)\nrf_pipe.fit(Train_X, Train_Y)\n# Setting params for TFIDF Vectorizer gridsearch\ntf_params = {\n 'tvec__max_features':[100, 2000],\n 'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n 'tvec__stop_words': [None, 'english'],\n \n}\n# Setting up randomforest params\nrf_params = {\n 'tvec__max_features':[2000],\n 'tvec__ngram_range': [(1, 2)],\n 'tvec__stop_words': ['english'],\n 'rf__max_depth': [1000],\n 'rf__min_samples_split': [100],\n 'rf__max_leaf_nodes': [None]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up GridSearch for Randomforest\nfrom sklearn.model_selection import GridSearchCV\nrf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 1, n_jobs = -1)\n# Setting up GridSearch for TFIDFVectorizer\ntvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = -1)\n# Fitting TVC GS\ntvc_gs.fit(Train_X, Train_Y)\n# Fitting Randomforest CV GS\nrf_gs.fit(Train_X, Train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tvc_gs.score(Train_X, Train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tvc_gs.score(Test_X, Test_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gs.score(Train_X, Train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gs.score(Test_X, Test_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\ndef build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units=512,kernel_initializer='uniform',activation='relu',input_dim=1000))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=6,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nclassifier = KerasClassifier(build_fn = build_classifier)\nparam_grid = dict(optimizer = ['Adam','Adagrad'],\n                  epochs=[10, ],\n                  batch_size=[16, 32])\ngrid = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy')\ngrid_result = grid.fit(x_train, Train_Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters = grid.best_params_\nbest_accuracy = grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"best_accuracy\",best_accuracy)\nprint(\"best parameter\",best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection, naive_bayes, svm\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[\"text_final\"]\nY = df['label']","execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8a2f604424b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_final\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/finalset/blooms.csv\",error_bad_lines = False)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[\"text_final\"], df[\"label\"],test_size=0.2)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n\nEncoder = LabelEncoder()\ny_train = Encoder.fit_transform(y_train)\ny_test = Encoder.fit_transform(y_test)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoder.inverse_transform(y_train)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"array(['analysis', 'synthesis', 'application', 'synthesis',\n       'comprehension', 'comprehension', 'evaluation.', 'knowledge',\n       'analysis', 'application', 'application', 'knowledge', 'analysis',\n       'knowledge', 'synthesis', 'knowledge', 'application', 'analysis',\n       'synthesis', 'analysis', 'knowledge', 'synthesis', 'analysis',\n       'synthesis', 'evaluation.', 'evaluation.', 'application',\n       'synthesis', 'synthesis', 'application', 'knowledge', 'synthesis',\n       'knowledge', 'comprehension', 'knowledge', 'analysis',\n       'comprehension', 'comprehension', 'synthesis', 'synthesis',\n       'comprehension', 'comprehension', 'evaluation.', 'knowledge',\n       'analysis', 'comprehension', 'knowledge', 'analysis', 'synthesis',\n       'synthesis', 'application', 'application', 'comprehension',\n       'synthesis', 'analysis', 'evaluation.', 'knowledge', 'synthesis',\n       'comprehension', 'application', 'analysis', 'synthesis',\n       'application', 'synthesis', 'evaluation.', 'synthesis',\n       'knowledge', 'application', 'application', 'application',\n       'application', 'comprehension', 'synthesis', 'analysis',\n       'comprehension', 'analysis', 'comprehension', 'synthesis',\n       'knowledge', 'application', 'synthesis', 'evaluation.',\n       'synthesis', 'analysis', 'evaluation.', 'knowledge', 'application',\n       'synthesis', 'comprehension', 'comprehension', 'comprehension',\n       'knowledge', 'analysis', 'application', 'application', 'synthesis',\n       'knowledge', 'synthesis', 'application', 'synthesis',\n       'evaluation.', 'analysis', 'analysis', 'synthesis', 'knowledge',\n       'synthesis', 'comprehension', 'analysis', 'analysis', 'analysis',\n       'comprehension', 'application', 'knowledge', 'application',\n       'synthesis', 'comprehension', 'application', 'application',\n       'application', 'evaluation.', 'comprehension', 'comprehension',\n       'analysis', 'evaluation.', 'application', 'application',\n       'knowledge', 'synthesis', 'evaluation.', 'synthesis', 'knowledge',\n       'application', 'evaluation.', 'comprehension', 'application',\n       'comprehension', 'comprehension', 'comprehension', 'application',\n       'synthesis', 'synthesis', 'evaluation.', 'comprehension',\n       'analysis', 'synthesis', 'knowledge', 'evaluation.', 'synthesis',\n       'analysis', 'comprehension', 'comprehension', 'comprehension',\n       'knowledge', 'evaluation.', 'evaluation.', 'application',\n       'synthesis', 'application', 'synthesis', 'evaluation.',\n       'knowledge', 'knowledge', 'knowledge', 'comprehension',\n       'comprehension', 'comprehension', 'comprehension', 'knowledge',\n       'knowledge', 'synthesis', 'analysis', 'evaluation.', 'knowledge',\n       'evaluation.', 'comprehension', 'evaluation.', 'evaluation.',\n       'application', 'comprehension', 'knowledge', 'application',\n       'comprehension', 'evaluation.', 'knowledge', 'knowledge',\n       'analysis', 'analysis', 'evaluation.', 'comprehension',\n       'comprehension', 'evaluation.', 'analysis', 'analysis',\n       'knowledge', 'synthesis', 'evaluation.', 'synthesis', 'knowledge',\n       'knowledge', 'analysis', 'evaluation.', 'evaluation.',\n       'comprehension', 'synthesis', 'evaluation.', 'knowledge',\n       'knowledge', 'synthesis', 'synthesis', 'comprehension',\n       'comprehension', 'knowledge', 'comprehension', 'comprehension',\n       'knowledge', 'analysis', 'synthesis', 'knowledge', 'evaluation.',\n       'application', 'evaluation.', 'synthesis', 'comprehension',\n       'comprehension', 'analysis', 'analysis', 'evaluation.',\n       'comprehension', 'synthesis', 'analysis', 'analysis', 'analysis',\n       'knowledge', 'synthesis', 'application', 'application',\n       'knowledge', 'comprehension', 'analysis', 'analysis',\n       'application', 'knowledge', 'synthesis', 'application', 'analysis',\n       'comprehension', 'evaluation.', 'knowledge', 'knowledge',\n       'comprehension', 'synthesis', 'evaluation.', 'knowledge',\n       'analysis', 'application', 'evaluation.', 'application',\n       'comprehension', 'application', 'analysis', 'knowledge',\n       'knowledge', 'evaluation.', 'knowledge', 'analysis', 'evaluation.',\n       'application', 'comprehension', 'synthesis', 'analysis',\n       'synthesis', 'analysis', 'application', 'analysis',\n       'comprehension', 'knowledge', 'analysis', 'knowledge',\n       'evaluation.', 'comprehension', 'synthesis', 'application',\n       'knowledge', 'application', 'application', 'application',\n       'evaluation.', 'comprehension', 'analysis', 'evaluation.',\n       'comprehension', 'application', 'comprehension', 'evaluation.',\n       'evaluation.', 'comprehension', 'comprehension', 'comprehension',\n       'synthesis', 'knowledge', 'analysis', 'comprehension', 'analysis',\n       'analysis', 'comprehension', 'application', 'synthesis',\n       'knowledge', 'synthesis', 'synthesis', 'analysis', 'application',\n       'analysis', 'synthesis', 'synthesis', 'knowledge', 'synthesis',\n       'comprehension', 'application', 'knowledge', 'application',\n       'comprehension', 'comprehension', 'knowledge', 'synthesis',\n       'application', 'evaluation.', 'analysis', 'synthesis', 'synthesis',\n       'synthesis', 'comprehension', 'analysis', 'knowledge', 'knowledge',\n       'comprehension', 'synthesis', 'comprehension', 'synthesis',\n       'evaluation.', 'application', 'knowledge', 'application',\n       'analysis', 'evaluation.', 'evaluation.', 'evaluation.',\n       'analysis', 'knowledge', 'comprehension', 'knowledge', 'analysis',\n       'synthesis', 'application', 'evaluation.', 'synthesis',\n       'comprehension', 'knowledge', 'synthesis', 'comprehension',\n       'evaluation.', 'knowledge', 'synthesis', 'comprehension',\n       'evaluation.', 'synthesis', 'synthesis', 'synthesis',\n       'application', 'synthesis', 'analysis', 'application',\n       'evaluation.', 'application', 'application', 'evaluation.',\n       'evaluation.', 'application', 'knowledge', 'comprehension',\n       'analysis', 'evaluation.', 'comprehension', 'knowledge',\n       'comprehension', 'comprehension', 'evaluation.', 'evaluation.',\n       'synthesis', 'analysis', 'evaluation.', 'application', 'knowledge',\n       'evaluation.', 'evaluation.', 'application', 'analysis',\n       'analysis', 'synthesis', 'analysis', 'application', 'evaluation.',\n       'analysis', 'evaluation.', 'evaluation.', 'synthesis', 'knowledge',\n       'application', 'knowledge', 'application', 'analysis', 'knowledge',\n       'evaluation.', 'analysis', 'application', 'evaluation.',\n       'synthesis', 'evaluation.', 'analysis', 'application', 'knowledge',\n       'application', 'synthesis', 'synthesis', 'evaluation.', 'analysis',\n       'analysis', 'evaluation.', 'evaluation.', 'synthesis',\n       'evaluation.', 'knowledge', 'analysis', 'evaluation.',\n       'comprehension', 'synthesis', 'analysis', 'evaluation.',\n       'application', 'application', 'application', 'comprehension',\n       'evaluation.', 'knowledge', 'comprehension', 'application',\n       'analysis', 'analysis', 'comprehension', 'analysis', 'knowledge',\n       'application', 'synthesis', 'analysis', 'evaluation.',\n       'comprehension', 'evaluation.', 'analysis', 'comprehension',\n       'evaluation.', 'comprehension', 'knowledge', 'evaluation.',\n       'knowledge', 'evaluation.', 'application', 'application',\n       'knowledge', 'evaluation.', 'application', 'application',\n       'comprehension', 'analysis', 'application', 'analysis'],\n      dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[]\nd=[i for i in X_train]\n\nfrom ast import literal_eval\n# ans = [i for i in d[0]]\n# print (ans)\n# ans[1]\n# # [i for i in ans]\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nporter=PorterStemmer()\ntext=[literal_eval(i) for i in d]\n\ntt=[]\ntt=[[str(j) for j in i[0]] for i in text]\nt=[]\nfor u in range(0,len(tt)):\n    t.append([tt[u]])\n# tt2=[j for j in tt[0]]\n\n# print(X_train)\n# print(tt)\n\nwordList= []\ni1=0\n# w[1000]=[]\nw={}\nfor i in tt:\n#     i = ngrams(i,4)\n#     wordList.append([])\n    w[i1]=[]\n    for j in i:\n        h=wordnet_lemmatizer.lemmatize(j)\n        \n        w[i1].append(porter.stem(h))\n#         w[i1].append(j)\n    i1=i1+1\n        \n# wordList\n# ans=[]\nans=[str(en) for en in w.values()]\n# tt2\nl=[]\nfor i in range(0,len(ans)):\n    l.append(str([literal_eval(ans[i])]))\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[]\nd=[i for i in X_test]\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\n\nfrom ast import literal_eval\n# ans = [i for i in d[0]]\n# print (ans)\n# ans[1]\n# # [i for i in ans]\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nporter=PorterStemmer()\nlemmatizer = WordNetLemmatizer()\ntext=[literal_eval(i) for i in d]\n\n\ntt=[[str(j) for j in i[0]] for i in text]\nt=[]\nfor u in range(0,len(tt)):\n    t.append([tt[u]])\n# tt2=[j for j in tt[0]]\n\n# print(X_train)\n# print(tt)\n\nwordList= []\ni1=0\n# w[1000]=[]\nw={}\nfor i in tt:\n#     print(i)\n#     i = ngrams(i,3)\n    \n\n#     wordList.append([])\n    w[i1]=[]\n    for j in i:\n        \n        h=wordnet_lemmatizer.lemmatize(j)\n        \n        w[i1].append(porter.stem(h))\n#          w[i1].append(j)\n    i1=i1+1\n        \n# wordList\n# ans=[]\nans=[str(en) for en in w.values()]\n# tt2\nl=[]\nfor i in range(0,len(ans)):\n    l.append(str([literal_eval(ans[i])]))\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.util import ngrams\n\n\n\ntokenize = ['I', 'am', 'aware', 'that', 'nltk', 'only', 'offers', 'bigrams', 'and', 'trigrams', ',', 'but', 'is', 'there', 'a', 'way', 'to', 'split', 'my', 'text', 'in', 'four-grams', ',', 'five-grams', 'or', 'even', 'hundred-grams']\nbigrams = ngrams(tokenize,2)\nstring=\" \".join(tt[0])\nngram=ngrams(tt[0],2)\nprint(Counter(ngram))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tt:\n#     print(i)\n    i = ngrams(i,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass TextSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, field):\n        self.field = field\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.field]\nclass NumberSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, field):\n        self.field = field\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[[self.field]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\ndef Tokenizer(str_input):\n    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n    porter_stemmer=nltk.PorterStemmer()\n    words = [porter_stemmer.stem(word) for word in words]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\n\nclass PosTagMatrix(BaseEstimator, TransformerMixin):\n    #normalise = True - devide all values by a total number of tags in the sentence\n    #tokenizer - take a custom tokenizer function\n    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):\n        self.tokenizer=tokenizer\n        self.normalize=normalize\n\n    #helper function to tokenize and count parts of speech\n    def pos_func(self, sentence):\n        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))\n\n    # fit() doesn't do anything, this is a transformer class\n    def fit(self, X, y = None):\n        return self\n\n    #all the work is done here\n    def transform(self, X):\n        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)\n        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)\n        if self.normalize:\n            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)\n\n        return X_tagged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\npipe1 = Pipeline([\n    ('cv', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('logit', LogisticRegression()),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe1.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport operator\npred = pipe1.predict_proba(X_test)\nlog_loss(y_test, pred)\n# index,value=max(pred[1],key=operator.itemgetter(1))\n# print (pred[1])\n# print(len(pred))\nt=[]\nfor j in range (0,len(pred)):\n#     print(pred)\n    v=max((pred[j]))\n    i=np.argmax(pred[j])\n#     print(i)\n    t.append(i)\n    \nt=np.asarray(t)\n\nt\n\nprint (\"Accuracy:\", accuracy_score(y_test, t))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_loss(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe1 = Pipeline([\n    ('cv', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    #('logit', LogisticRegression()),\n    ('bnb', BernoulliNB()),\n   \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport operator\npipe1.fit(X_train,y_train)\npred = pipe1.predict_proba(X_test)\nlog_loss(y_test, pred)\n# index,value=max(pred[1],key=operator.itemgetter(1))\n# print (pred[1])\n# print(len(pred))\nt=[]\nfor j in range (0,len(pred)):\n#     print(pred)\n    v=max((pred[j]))\n    i=np.argmax(pred[j])\n#     print(i)\n    t.append(i)\n    \nt=np.asarray(t)\n\nt\n\nprint (\"Accuracy:\", accuracy_score(y_test, t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\n\nclass PosTagMatrix(BaseEstimator, TransformerMixin):\n    #normalise = True - devide all values by a total number of tags in the sentence\n    #tokenizer - take a custom tokenizer function\n    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):\n        self.tokenizer=tokenizer\n        self.normalize=normalize\n\n    #helper function to tokenize and count parts of speech\n    def pos_func(self, sentence):\n        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))\n\n    # fit() doesn't do anything, this is a transformer class\n    def fit(self, X, y = None):\n        return self\n\n    #all the work is done here\n    def transform(self, X):\n        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)\n        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)\n        if self.normalize:\n            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)\n\n        return X_tagged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n#params are from the above mentioned tutorial\nxgb_params={\n    'objective': 'multi:softprob',\n    'eta': 0.1,\n    'max_depth': 3,\n    'silent' :1,\n    'num_class' : 6,\n    'eval_metric' : \"mlogloss\",\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.3,\n    'seed':17,\n    'num_rounds':2000,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\npipe1 = Pipeline([\n    ('u1', FeatureUnion([\n        ('tfdif_features', Pipeline([\n            ('cv', CountVectorizer()),\n            ('tfidf', TfidfTransformer()),\n#             ('tfidf_logit', ClassifierWrapper(LogisticRegression())),\n        ])),\n#         ('pos_features', Pipeline([\n#             ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n#             ('pos_logit', ClassifierWrapper(LogisticRegression())),\n#         ])),\n    ])),\n    \n    ('xgb', XGBClassifier(**xgb_params)),\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals import joblib\njoblib.dump(pipe1, 'filename.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport operator\npipe1.fit(X_train,y_train)\npred = pipe1.predict_proba(X_test)\nlog_loss(y_test, pred)\n# ## index,value=max(pred[1],key=operator.itemgetter(1))\n# # print (pred[1])\n# # print(len(pred))\nt=[]\nfor j in range (0,len(pred)):\n# ##     print(pred)\n# v=max((pred[j]))\n    i=np.argmax(pred[j])\n# # #     print(i)\n    t.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint (\"Accuracy:\", accuracy_score(y_test, t))\nprint(classification_report(y_test, t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}